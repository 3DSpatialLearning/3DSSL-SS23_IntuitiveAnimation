# [VOCA: Voice Operated Character Animation](https://voca.is.tue.mpg.de)

This is an extension of the official [VOCA](https://voca.is.tue.mpg.de) repository.

<p align="center"> 
<img src="gif/speech_driven_animation.gif">
</p>

VOCA is a simple and generic speech-driven facial animation framework that works across a range of identities. This codebase demonstrates how to synthesize realistic character animations given an arbitrary speech signal and a static character mesh. For details please see the scientific publication

```
Capture, Learning, and Synthesis of 3D Speaking Styles.
D. Cudeiro*, T. Bolkart*, C. Laidlaw, A. Ranjan, M. J. Black
Computer Vision and Pattern Recognition (CVPR), 2019
```

A pre-print of the publication can be found [here](
https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/510/paper_final.pdf).
You can also check out the [VOCA Blender Addon](https://github.com/SasageyoOrg/voca-blender)

## Video

See the demo video for more details and results.

[![VOCA](https://img.youtube.com/vi/XceCxf_GyW4/0.jpg)](https://youtu.be/XceCxf_GyW4)

While showing good approximations of the lip movements, no upper face movements and hence, no emotional expressions are achieved.
This approach aims to overcome this limitation by first applying post-processing in a first step and retraining the pipeline on a generated dataset in a second step.
Please note that retraining on emotional data does not work for allowing for emotional expressions. This work is mainly made public in order to make future work on the official [VOCA](https://voca.is.tue.mpg.de) repository easier.

## Set-up

comment: replace with our conda env set-up instructions!

The code uses Python 3.6.8 and it was tested on Tensorflow 1.14.0.

Install pip and virtualenv
```
sudo apt-get install python3-pip python3-venv
```

Install ffmpeg
```
sudo apt install ffmpeg
```

Clone the git project:
```
git clone https://github.com/TimoBolkart/voca.git
```

Set up virtual environment:
```
mkdir <your_home_dir>/.virtualenvs
python3 -m venv <your_home_dir>/.virtualenvs/voca
```

Activate virtual environment:
```
cd voca
source <your_home_dir>/voca/bin/activate
```

Make sure your pip version is up-to-date:
```
pip install -U pip
```

The requirements (including tensorflow) can be installed using:
```
pip install -r requirements.txt
```

Install mesh processing libraries from [MPI-IS/mesh](https://github.com/MPI-IS/mesh) within the virtual environment.

Please refer to VOCA for the setup of the data, a general introduction and other demonstrations.

## Post-Processing

For post-processing run ...

## Training

In order to train on emotional content 

### Data Generation

## License

Free for non-commercial and scientific research purposes. By using this code, you acknowledge that you have read the license terms (https://voca.is.tue.mpg.de/license.html), understand them, and agree to be bound by them. If you do not agree with these terms and conditions, you must not use the code.


## Referencing VOCA

If you find this code useful for your research, or you use results generated by VOCA in your research, please cite following paper:

```
@article{VOCA2019,
    title = {Capture, Learning, and Synthesis of {3D} Speaking Styles},
    author = {Cudeiro, Daniel and Bolkart, Timo and Laidlaw, Cassidy and Ranjan, Anurag and Black, Michael},
    journal = {Computer Vision and Pattern Recognition (CVPR)},
    pages = {10101--10111},
    year = {2019}
    url = {http://voca.is.tue.mpg.de/}
}
```

## Acknowledgement

We thank Raffi Enficiaud and Ahmed Osman for pushing the release of psbody.mesh.









